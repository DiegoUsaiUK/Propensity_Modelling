---
title: "Using h2o and DALEX to Estimate Likelihood to Purchase a Financial Product"
subtitle:  "Propensity Modelling - Part 1 of 3: Data Preparation and Exploratory Data Analysis"
author: "Diego Usai"
date: "14 February 2020"
output:
  html_document:
    theme: spacelab
    # df_print: paged
    highlight: pygments
    number_sections: false
    toc: true
    toc_float: true
    toc_depth : 4
    font-family: Roboto
    code_folding: none
    keep_md: false
    dpi: 300
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  eval       = TRUE,   # TRUE to evaluate every single chunck
  warning    = FALSE,  # FALSE to suppress warnings from being shown
  message    = FALSE,  # FALSE to avoid package loading messages
  cache      = TRUE,  # TRUE to save every single chunck to a folder
  echo       = TRUE,   # TRUE to display code in output document
  out.width  = "100%",
  out.height = "100%",
  fig.align  = "center"
)
```

```{r switch off locale, include=FALSE}
# turn off locale-specific sorting to get output messages in English
Sys.setlocale("LC_TIME", "C")
```

```{r libraries}
library(tidyverse)
library(data.table)
library(skimr)
library(correlationfunnel)
library(GGally)
library(ggmosaic)
library(knitr)
```

```{r sourcing scripts, include=FALSE}
source("../02_scripts/plot_hist_funct.R")
source("../02_scripts/plot_ggpairs_funct.R")
```


## Introduction

In this day and age, a business that leverages data to understand the drivers of its customers' behaviour has a true competitive advantage. Organisations can dramatically improve their performance in the market by analysing customer level data in an effective way and focus their efforts towards those that are more likely to engage. 

One trialled and tested approach to tease out this type of insight is [__Propensity Modelling__](https://en.wikipedia.org/wiki/Predictive_modelling), which combines information such as a __customers’ demographics__ (age, race, religion, gender, family size, ethnicity, income, education level), __psycho-graphic__ (social class, lifestyle and personality characteristics), __engagement__ (emails opened, emails clicked, searches on mobile app, webpage dwell time, etc.), __user experience__ (customer service phone and email wait times, number of refunds, average shipping times), and __user behaviour__ (purchase value on different time-scales, number of days since most recent purchase, time between offer and conversion, etc.) to estimate the likelihood of a certain customer profile to performing a certain type of behaviour (e.g. the purchase of a product).

Once you understand the probability of a certain customer to interact with your brand, buy a product or a sign up for a service, you can use this information to create scenarios, be it minimising __marketing expenditure__, maximising __acquisition targets__, and optimise __email send frequency__ or __depth of discount__.


## Project Structure

In this project I'm analysing the results of a bank __direct marketing campaign__ to sell term deposits in order to identify what type of customer is more likely to respond. The marketing campaigns were based on phone calls and more than one contact to the same client was required at times. 

First, I am going to carry out an __extensive data exploration__ and use the results and insights to prepare the data for analysis.

Then, I'm __estimating a number of models__ and assess their performance and fit to the data using a __model-agnostic methodology__ that enables to __compare traditional "glass-box" models and "black-box" models__.

Last, I'll fit __one final model__ that combines findings from the exploratory analysis and insight from models' selection and use it to __run a revenue optimisation__.


## The data

The Data is the [__Portuguese Bank Marketing__](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) set from the [__UCI Machine Learning Repository__](https://archive.ics.uci.edu/ml/datasets) and describes the direct marketing campaigns carried out by a Portuguese banking institution aimed at selling term deposits/certificate of deposits to their customers. The marketing campaigns were based on phone calls to potential buyers from May 2008 to November 2010.

Of the four variants of the datasets available on the UCI repository, I've chosen the [__bank-additional-full.csv__](https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip) which contains 41,188 examples with 21 different variables (10 continuous, 10 categorical plus the target variable). A full description of the variables is provided in the appendix. 

In particular, the target __subscribed__ is a __binary response variable__ indicating whether the client subscribed (‘Yes’ or numeric value 1) to a term deposit or not (‘No’ or numeric value 0), which make this a [__binary classification problem__](https://en.wikipedia.org/wiki/Binary_classification).



## Loading data and initial inspection

The data I'm using ( __bank-direct-marketing.csv__) is a modified version of the full set mentioned earlier and can be found on my [__GitHub profile__](https://github.com/DiegoUsaiUK/Customer_Marketing_Engagement/tree/master/01_data). As it contains lots of double quotation marks, some manipulation is required to get into a usable format. 

First, I load each row into one string
```{r raw data}
data_raw <- 
   data.table::fread(
      file = "../01_data/bank_direct_marketing_modified.csv",
      # use character NOT present in data so each row collapses to a string
      sep = '~',
      quote = '',
      # include headers as first row
      header = FALSE
   )
```

Then, clean data by removing double quotation marks, splitting row strings into single variables and select target variable `subscribed` to sit on the left-hand side as first variable in data set
```{r}
data_clean <- 
   # remove all double quotation marks "
   as_tibble(sapply(data_raw, function(x) gsub("\"", "", x))) %>% 
   # split out into 21 variables 
   separate(col    = V1,
            into   = c('age', 'job', 'marital', 'education', 'default', 
                       'housing', 'loan', 'contact', 'month', 'day_of_week', 
                       'duration', 'campaign', 'pdays', 'previous',
                       'poutcome', 'emp_var_rate', 'cons_price_idx',
                       'cons_conf_idx', 'euribor3m', 'nr_employed', 'subscribed'),
            # using semicolumn as separator
            sep    = ";",
            # to drop original field
            remove = T) %>% 
   # drop first row, which contains 
   slice((nrow(.) - 41187):nrow(.)) %>% 
   # move targer variable subscribed to be first variable in data set
   select(subscribed, everything()) 
```


## Initial Data Manipulation

Let's have a look! 

All variables are set as character and some need adjusting.
```{r, collapse=T}
data_clean %>% glimpse()
```

I'll start with setting the variables that are continuous in nature to numeric and change `pdays` 999 to 0 (999 means client was not previously contacted). I'm also shortening level names of some categorical variables to ease visualisations.

Note that, although numeric in nature, `campaign` is more of a categorical variable so I am leaving it as a character.
```{r}
data_clean <- 
   data_clean %>%
  
    # recoding the majority class as 0 and the minority class as 1
    mutate(subscribed = case_when(subscribed == 'no' ~ 0, 
                                                TRUE ~ 1) %>% 
              as_factor) %>% 
  
    # change continuous variables that are numeric to type double
    mutate_at(c('age','duration', 'pdays', 'previous',
                'emp_var_rate', 'cons_price_idx', 'cons_conf_idx',
                'euribor3m', 'nr_employed'),
                 as.double) %>% 
  
    # change pdays 999 to 0 (zero)
    mutate(pdays = case_when(pdays == '999' ~ 0,
                            TRUE ~ pdays),
          
    # shortening level names of some categ. vars to ease visualisations
    job = case_when(
                    job ==  'housemaid'     ~ 'maid',
                    job ==  'services'      ~ 'svcs',				
                    job ==  'admin.'        ~	'adm',	
                    job ==  'blue-collar'		~	'bcol',
                    job ==  'technician'		~	'tech',
                    job ==  'retired'				~ 'ret',
                    job ==  'management'		~	'mgmt',
                    job ==  'unemployed'		~	'uemp',
                    job ==  'self-employed'	~	'self',
                    job ==  'unknown'       ~ 'unk',
                    job ==  'entrepreneur'	~	'entr',
                    TRUE                    ~ 'stdn'),

    marital = case_when(
                    marital == 'married'  ~ 'mar',				
                    marital == 'single'   ~ 'sig',				
                    marital == 'divorced' ~ 'div',				
                    TRUE                  ~ 'unk'),

    education = case_when(
                    education ==  'basic.4y'            ~ '4y',
                    education ==  'basic.6y'            ~ '6y',				
                    education ==  'basic.9y'            ~	'9y',	
                    education ==  'high.school'		      ~	'hs',
                    education ==  'professional.course'	~	'crse',
                    education ==  'unknown'				      ~ 'unk',
                    education ==  'university.degree'		~	'uni',
                    TRUE                                ~ 'ilt'),

    default = case_when(
                    default == 'unknown' ~ 'unk',
                    default == 'yes'     ~ 'yes',
                    TRUE                 ~ 'no'),

    contact = case_when(
                    contact == 'telephone' ~ 'tel',
                    contact == 'cellular'  ~ 'mob'),

    poutcome = case_when(
                    poutcome == 'nonexistent' ~ 'non',
                    poutcome == 'failure'     ~ 'fail',
                    TRUE                      ~ 'scs'),
    housing = case_when(
                    housing == 'unknown' ~ 'unk',
                    default == 'yes'     ~ 'yes',
                    TRUE                 ~ 'no'),
    loan = case_when(
                    loan == 'unknown' ~ 'unk',
                    default == 'yes'  ~ 'yes',
                    TRUE              ~ 'no')
    )
```

There are no missing values in any of the variables (continuous or categorical) in this data set. For that reason, no imputation is necessary.


```{r}
data_clean %>% 
  skimr::skim()
```

__NOTE__: I've left all categorical variables as unordered as __h2o__ (which I'm going to be using for modelling) does not support ordered categorical variables


## Exploratory Data Analysis 


Although an integral part of any Data Science project and crucial to the full success of the analysis, [Exploratory Data Analysis (EDA)](https://en.wikipedia.org/wiki/Exploratory_data_analysis) can be an incredibly labour intensive and time consuming process. Recent years have seen a proliferation of approaches and libraries aimed at speeding up the process and in this project I'm going to sample one of the "new kids on the block" ( the [__correlationfunnel__](https://business-science.github.io/correlationfunnel/) ) and combine its results with a more traditional EDA. 


### _correlationfunnel_

`correlationfunnel` is a package developed with the aim to speed up Exploratory Data Analysis (EDA), a process that can be very time consuming even for small data sets.

With 3 simple steps we can produce a graph that arranges predictors top to bottom in descending order of absolute correlation with the target variable. Features at the top of the funnel are expected to have have stronger predictive power in a model.

This approach offers a quick way to identify a hierarchy of expected predictive power for all variables and gives an early indication of which predictors should feature strongly/weakly in any model. 

```{r}
data_clean %>%  
  
  # turn numeric and categorical features into binary data
  binarize(n_bins = 4, # bin number for converting features to discrete 
           thresh_infreq = 0.01 # thresh. for assign categ. features into "Other"
          ) %>%
  
  # Correlate target variable to features in data set 
  correlate(target = subscribed__1) %>% 
  
  # correlation funnel visualisation
  plot_correlation_funnel()
```




Zooming in on the top 5 features we can see that certain characteristics have a greater correlation with the target variable (subscribing to the term deposit product) when: 

* The `duration` of the last phone contact with the client is 319 seconds or longer
* The number of `days` that passed by after the client was last contacted is greater than 6
* The outcome of the `previous` marketing campaign was `success`
* The number of employed is 5,099 thousands or higher
* The value of the euribor 3 month rate is 1.344 or higher

```{r}
data_clean %>%
  select(subscribed, duration, pdays, poutcome, nr_employed, euribor3m) %>%
  binarize(n_bins = 4, # bin number for converting numeric features to discrete 
           thresh_infreq = 0.01 # thresh. for assign categ. features into "Other"
          ) %>%
  # Correlate target vriable to features in data set 
  correlate(target = subscribed__1) %>% 
  plot_correlation_funnel(limits = c(-0.4, 0.4))
```



Conversely, variables at the bottom of the funnel, such as __day_of_week__, __housing__, and __loan__. show very little variation compared to the target variable (i.e.: they are very close to the zero correlation point to the response). For that reason, I'm not expecting these features to impact the response.


```{r}
data_clean %>%
  select(subscribed, education, campaign, day_of_week, housing, loan) %>%
  binarize(n_bins = 4, # bin number for converting numeric features to discrete 
           thresh_infreq = 0.01 # thresh. for assign categ. features into "Other"
          ) %>%
  # Correlate target vriable to features in data set 
  correlate(target = subscribed__1) %>% 
  plot_correlation_funnel(limits = c(-0.4, 0.4))
```


### Features exploration

Guided by the results of this visual correlation analysis, I will continue to explore the relationship between the target and each of the predictors in the next section. For this I will enlist the help of the brilliant __GGally__ library to visualise a modified version of the correlation matrix with `Ggpairs`, and plot `mosaic charts` with the __ggmosaic__ package, a great way to examine the relationship among two or more categorical variables.



#### Target Variable

First things first, the __target variable__: `subscribed` shows a __strong class imbalance__, with nearly 89% in the __No category__ to 11% in the __Yes category__.

```{r}
data_clean %>% 
  select(subscribed) %>% 
  group_by(subscribed) %>% 
  count() %>%
  # summarise(n = n()) %>% # alternative to count() - here you can name it!
  ungroup() %>% 
  mutate(perc = n / sum(n)) %>% 
  
  ggplot(aes(x = subscribed, y = n, fill = subscribed) ) + 
  geom_col() +
  geom_text(aes(label = scales::percent(perc, accuracy = 0.1)),
            nudge_y = -2000,
            size = 4.5) +
  theme_minimal() +
  theme(legend.position = 'none',
        plot.title    = element_text(hjust = 0.5)) +
  labs(title = 'Target Variable',
        x = 'Subscribed', 
        y = 'Number of Responses')
```


I will address __class imbalance__  before entering the modelling phase with a __re-sampling technique__, which aims at rebalancing the dataset by "shrinking" the prevalent class ("No" or 0). This is to ensure that the model adequately detect what variables are driving the ‘yes’ and ‘no’ responses.



#### Predictors

Let's continue with some of the numerical features:

```{r, some continuous features}
data_clean %>% 
   select(subscribed, duration, age, pdays, previous) %>% 
   plot_ggpairs_funct(colour = subscribed)
```

Although the correlation funnel analysis revealed that __duration__ has the strongest expected predictive power, it is unknown before a call (it’s obviously known afterwards) and offers very little actionable insight or predictive value. Therefore, it should be discarded from any realistic predictive model and will not be used in this analysis.

__age__ 's density plots have very similar variance compared to the target variable and are centred around the same area. For these reasons, it should not have a great impact on __subscribed__.

Despite continuous in nature, __pdays__ and __previous__ are in fact categorical features and are also all strongly right skewed. For these reasons, they will need to be discretised into groups. Both variables are also moderately correlated, suggesting that they may capture the same behaviour.

Next, I visualise the bank client data with the __mosaic charts__:

```{r, bank client data all}
job <- ggplot(data = data_clean) +
   geom_mosaic(aes(x = product(job, subscribed), fill = job)) +
   theme_minimal() +
   theme(legend.position = 'none', 
         plot.title = element_text(hjust = 0.5) ) +
   labs(x = '', y = '', title = 'Job') 

mar <- ggplot(data = data_clean) +
   geom_mosaic(aes(x = product(marital, subscribed), fill = marital)) +
   theme_minimal() +
   theme(legend.position = 'none', 
         plot.title = element_text(hjust = 0.5) ) +
   labs(x = '', y = '', title = 'Marital')

edu <- ggplot(data = data_clean) +
   geom_mosaic(aes(x = product(education, subscribed), fill = education)) +
   theme_minimal() +
   theme(legend.position = 'none', 
         plot.title = element_text(hjust = 0.5) ) +
   labs(x = '', y = '', title = 'Education')

def <- ggplot(data = data_clean) +
   geom_mosaic(aes(x = product(default, subscribed), fill = default)) +
   theme_minimal() +
   theme(legend.position = 'none', 
         plot.title = element_text(hjust = 0.5) ) +
   labs(x = '', y = '', title = 'Default') 

hou <- ggplot(data = data_clean) +
   geom_mosaic(aes(x = product(housing, subscribed), fill = housing)) +
   theme_minimal() +
   theme(legend.position = 'none', 
         plot.title = element_text(hjust = 0.5) ) +
   labs(x = '', y = '', title = 'Housing')

loa <- ggplot(data = data_clean) +
   geom_mosaic(aes(x = product(loan, subscribed), fill = loan)) +
   theme_minimal() +
   theme(legend.position = 'none', 
         plot.title = element_text(hjust = 0.5) ) +
   labs(x = '', y = '', title = 'Loan')


gridExtra::grid.arrange(job, mar, hou, edu, def, loa, nrow = 2)
```


In line with the _correlationfunnel_ findings, __job__, __education__, __marital__ and __default__ all show a good level of variation compared to the target variable, indicating that they would impact the response. In contrast, __housing__ and __loan__ sat at the very bottom of the funnel and are expected to have little influence on the target, given the small variation when split by "subscribed" response. 

__default__ has only 3 observations in the ‘yes’ level, which will be rolled into the least frequent level as they're not enough to make a proper inference. Level ‘unknown’ of the __housing__ and __loan__ variables have a small number of observations and will be rolled into the second smallest category. Lastly, __job__ and __education__ would also benefit from grouping up of least common levels.


Moving on to the other campaign attributes:
```{r, other campaign attributes}
data_clean %>% 
   select(subscribed, campaign, poutcome) %>% 
   plot_ggpairs_funct(colour = subscribed)
```

Although continuous in principal, __campaign__ is more categorical in nature and strongly right skewed, and will need to be discretised into groups. However, we have learned from the earlier correlation analysis that is not expected be a strong drivers of variation in any model.

On the other hand, __poutcome__ is one of the attributes expected to be have a strong predictive power. The uneven distribution of levels would suggest to roll the least common occurring level (__success__ or `scs`) into another category. However, contacting a client who previously purchased a term deposit is one of the catacteristics with highest predictive power and needs to be left ungrouped. 


Then, I'm looking at last contact information:
```{r last contact information}
con <- ggplot(data = data_clean) +
   geom_mosaic(aes(x = product(contact, subscribed), fill = contact)) +
   theme_minimal() +
   theme(legend.position = 'none', 
         plot.title = element_text(hjust = 0.5) ) +
   labs(x = '', y = '', title = 'Contact') 

mth <- ggplot(data = data_clean) +
   geom_mosaic(aes(x = product(month, subscribed), fill = month)) +
   theme_minimal() +
   theme(legend.position = 'none', 
         plot.title = element_text(hjust = 0.5) ) +
   labs(x = '', y = '', title = 'Month')

dow <- ggplot(data = data_clean) +
   geom_mosaic(aes(x = product(day_of_week, subscribed), fill = day_of_week)) +
   theme_minimal() +
   theme(legend.position = 'none', 
         plot.title = element_text(hjust = 0.5) ) +
   labs(x = '', y = '', title = 'Day of Week')

gridExtra::grid.arrange(con, mth, dow, nrow = 2)
```

__contact__ and __month__ should impact the response variable as they both have a good level of variation compared to the target. __month__ would also benefit from grouping up of least common levels.

In contrast, __day_of_week__ does not appear to impact the response as there is not enough variation between the levels.

Last but not least, the social and economic attributes:
```{r, social-economic context attributes}
data_clean %>% 
   select(subscribed, emp_var_rate, cons_price_idx, 
          cons_conf_idx, euribor3m, nr_employed) %>% 
   plot_ggpairs_funct(colour = subscribed)
```

All __social and economic attributes__ show a good level of variation compared to the target variable, which suggests that they should all impact the response. They all display a high degree of multi-modality and do not have an even spread through the density plot, and will need to be binned. 

It is also worth noting that, with the exception of _cons_confidence_index_, all other social and economic attributes are strongly correlated to each other, indicating that only one could be included in the model as they are all “picking up” similar economic trend.


## Data Processing and Transformation

Following up on the findings from the Exploratory Data Analysis, I'm getting the data ready for modelling.


### Discretising of categorical predictors

Here, I'm using a helper function [`plot_hist_funct`](https://github.com/DiegoUsaiUK/Customer_Marketing_Engagement/tree/master/03_scripts) to take a look at features histograms. That helps understanding how to combine least common levels into "other' category.
```{r}
data_clean %>%
    select_if(is_character) %>%
    plot_hist_funct()
```

With the exception of __day_of_week__ and __contact__, all categorical variables need some grouping up. I'm going to go through the first one as an example of how I approached the problem and include all changes made at the end.

#### Example with _marital_ status

A 3-bin combination seems sensible for the __marital__ status category
```{r}
data_clean %>%
    # combine least common factors into "other' category
    select(marital) %>% 
    mutate(marital_binned = marital %>% fct_lump( 
                              # n = how many categs to keep
                              n = 2,  
                              # name other category
                              other_level = "other"
                              )) %>% 
  plot_hist_funct()
```

### Discretising of continuous variables

Using the same approach as to categorical variables, I'm plotting all numerical features.

```{r}
data_clean %>%
    select_if(is.numeric) %>%
    plot_hist_funct()
```

All continuous variables can benefit from some grouping. For simplicity and speed, I'm using the bins calculated by the `correlationfunnel` package. __duration__ will not be processed as I'm NOT including it in any of my models.

#### Example with _consumer confidence index_

A 3-level binning seems sensible for `cons_price_idx`
```{r}
data_clean %>%
    select(cons_price_idx) %>% 
    mutate(cons_price_idx_binned = case_when(
            between(cons_price_idx, -Inf, 93.056)   ~ "Inf_93.056",
            between(cons_price_idx, 93.056, 93.912) ~ "93.056_93.912",
            TRUE                                    ~  "93.913_Inf")) %>%  
  
    plot_hist_funct()
```

I create now a `data_final` file with all the binned variables, set all categorical variables to factors and take a good look at all of them.

```{r}
data_final <- 
  data_clean %>%
    # removing duration, which I'm not going to use for modelling 
    select(-duration) %>% 
    # applying grouping
    mutate(
        job        = job %>% fct_lump(n = 11, other_level = "other"),
        marital    = marital %>% fct_lump(n = 2, other_level = "other"),
        education  = education %>% fct_lump(n = 6, other_level = "other"),
        default    = default %>% fct_lump(n = 1, other_level = "other"),
        housing    = housing %>% fct_lump(n = 1, other_level = "other"),
        loan       = loan %>% fct_lump(n = 1, other_level = "other"),
        # month      = month %>% fct_lump(n = 6,other_level = "other"),
        campaign   = campaign %>% fct_lump(n = 3, other_level = "other"),
        # poutcome = poutcome %>% fct_lump(n = 1, other_level = "other"),
        pdays = case_when(
            pdays == 0 ~ "Never",
            TRUE       ~ "Once_or_more"),
        previous = case_when(
            previous == 0 ~ "Never",
            TRUE          ~ "Once_or_more"),
        emp_var_rate = case_when(
            between(emp_var_rate, -Inf, -1.8) ~ "nInf_n1.8",
            between(emp_var_rate, -1.9, -0.1) ~ "n1.9_n0.1",
            TRUE                              ~  "n0.2_Inf"),
        cons_price_idx = case_when(
            between(cons_price_idx, -Inf, 93.056)   ~ "nInf_93.056",
            between(cons_price_idx, 93.057, 93.912) ~ "93.057_93.912",
            TRUE                                    ~  "93.913_Inf"),
        cons_conf_idx = case_when(
            between(cons_conf_idx, -Inf, -46.19)  ~ "nInf_n46.19",
            between(cons_conf_idx, -46.2, -41.99) ~ "n46.2_n41.9",
            between(cons_conf_idx, -42.0, -39.99) ~ "n42.0_n39.9",
            between(cons_conf_idx, -40.0, -36.39) ~ "n40.0_n36.4",
            TRUE                                  ~ "n36.5_Inf"),
        euribor3m = case_when(
            between(euribor3m, -Inf, 1.298)  ~ "nInf_1.298",
            between(euribor3m, 1.299, 4.190) ~ "1.299_4.190",
            between(euribor3m, 1.191, 4.864) ~ "1.299_4.864",
            between(euribor3m, 1.865, 4.862) ~ "1.299_4.962",
            TRUE                             ~ "4.963_Inf"),
        nr_employed = case_when(
            between(nr_employed, -Inf, 5099.1)    ~ "nInf_5099.1",
            between(nr_employed, 5099.1, 5191.01) ~ "5099.1_5191.01",
            TRUE                                  ~ "5191.02_Inf")
           ) %>% 
  
   # change categorical variables to factors 
   mutate_at(c('contact', 'month', 'day_of_week', 'pdays', 'poutcome', 
               'previous', 'emp_var_rate', 'cons_price_idx', 
               'cons_conf_idx', 'euribor3m', 'nr_employed'),
             as.factor)
```


It all looks fine!
```{r, collapse=T}
data_final %>% str()
```


## Summary of Exploratory Data Analysis & Preparation

- Correlation analysis with __correlationfunnel__ helped identify a hierarchy of expected predictive power for all variables

- __duration__ (eqpt_rent) has strongest correlation with target variable whereas some of the bank client data like __housing__ and __loan__ shows the weakest correlation

- However, __duration__ will __NOT__ be used in the analysis as it is unknown before a call. As such it offers very little actionable insight or predictive value and should be discarded from any realistic predictive model

- The target variable __subscribed__ shows strong class imbalance, with nearly 89% of __No churn__, which will need to be addresses before the modelling analysis can begin

- Most predictors benefited from grouping up of least common levels

- Further feature exploration revealed the most __social and economic context attributes__ 
are strongly correlated to each other, suggesting that only a selection of them could be considered in a final model



### Save final dataset

Lastly, I save the `data_final` set for the next phase of the analysis.
```{r, eval=F}
# Saving clensed data for analysis phase
saveRDS(data_final, "../01_data/data_final.rds")
```



### Code Repository
The full R code and all relevant files can be found on my GitHub profile @ [__Propensity Modelling__](https://github.com/DiegoUsaiUK/Propensity_Modelling) 


### References

* For the original paper that used the data set see: [__A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems__](http://repositorium.sdum.uminho.pt/bitstream/1822/30994/1/dss-v3.pdf), S. Moro, P. Cortez and P. Rita. 

* To Speed Up Exploratory Data Analysis see: [__correlationfunnel Package Vignette__](https://business-science.github.io/correlationfunnel/) 



## Appendix

### Table 1 – Variables Description

Category       | Attribute    | Description                   | Type
:--------------|:-------------|:------------------------------|:------------------------------
Target       | subscribed   | has the client subscribed a term deposit?   | binary: "yes","no"
Client Data  | age          | -                    | numeric
Client Data  | job          |type of job           | categorical
Client Data  | marital      | marital status       |categorical
Client Data  | education    | -                    |categorical
Client Data  | default      | has credit in default?  |categorical: "no","yes","unknown"
Client Data  | housing      | has housing loan?  |categorical: "no","yes","unknown"
Client Data  | loan         | has personal loan?  |categorical:"no","yes","unknown"
Last Contact Info  |contact      |contact communication type         | categorical:"cellular","telephone"
Last Contact Info  |month        |last contact month of year         | categorical
Last Contact Info  |day_of_week  | last contact day of the week    |categorical: "mon","tue","wed","thu","fri"
Last Contact Info  |duration     | last contact duration, in seconds | numeric
Campaigns attrib. |campaign     | number of contacts during this campaign and for this client | numeric
Campaigns attrib. |pdays        | number of days after client was last contacted from previous campaign | numeric; 999 means client was not previously contacted
Campaigns attrib. |previous     | number of contacts before this campaign and for this client | numeric
Campaigns attrib. |poutcome     | outcome of previous marketing campaign | categorical: "failure","nonexistent","success"
Social & Economic |emp.var.rate | employment variation rate - quarterly indicator | numeric
Social & Economic |cons.price.idx | consumer price index - monthly indicator | numeric
Social & Economic |cons.conf.idx | consumer confidence index - monthly indicator | numeric
Social & Economic |euribor3m    | euribor 3 month rate - daily indicator | numeric
Social & Economic |nr.employed  | number of employees - quarterly indicator | numeric












